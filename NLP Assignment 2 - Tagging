{"cells":[{"cell_type":"markdown","metadata":{"id":"tLHeV9icIk-b"},"source":["![](https://i.imgur.com/qkg2E2D.png)\n","\n","# Natural Language Processing\n","\n","## Assignment 002 - POS Tagging with Universal Dependencies\n","\n","> Notebook by:\n","> - NLP Course Stuff\n","\n","## Revision History\n","\n","| Version | Date       | User        |Content / Changes                                                   |\n","|---------|------------|-------------|--------------------------------------------------------------------|\n","| 0.2.000 | 15/05/2024 | course staff| Updated `forward` method (Part 3) documentation to clarify return values as log probabilities. |\n","| 0.1.000 | 01/05/2024 | Course stuff | First version                                                      |\n","|         |            |             |                                                                    |"]},{"cell_type":"markdown","metadata":{"id":"JCFoQxO0H0yk"},"source":["## Overview\n","In this assignment, you will train and evaluate a Part-of-Speech (POS) tagger using data from the Universal Dependencies (UD) project. POS taggers assign parts of speech to each word in a sentence, such as noun, verb, adjective, etc., which are crucial for many natural language processing tasks.\n","\n","## Dataset\n","Utilize the English Web Treebank from the Universal Dependencies project. You can access and explore the dataset [here](https://universaldependencies.org/). For a better understanding of the project and the data format, visit the [introduction page](https://universaldependencies.org/introduction.html).\n","\n","## Tasks Overview\n","\n","#### Part 1: Dataset Preparation\n","- **Objective**: Get the Universal Dependencies dataset ready for the tagging tasks.\n","- **Activities**: Download, preprocess, and format the dataset.\n","\n","#### Part 2: HMM Tagger\n","- **Objective**: Create and assess a POS tagger using the Hidden Markov Model.\n","- **Activities**: Construct, train, and evaluate the HMM tagger.\n","\n","#### Part 3: Feed-Forward Neural Network Tagger\n","- **Objective**: Build a POS tagger using a feed-forward neural network with word embeddings.\n","- **Activities**: Develop and train the model in PyTorch, then evaluate its effectiveness.\n","\n","#### Part 4: NLTK MEMM Tagger\n","- **Objective**: Implement and test a MEMM-based POS tagger using NLTK.\n","- **Activities**: Train the MEMM tagger, then evaluate its performance.\n","\n","#### Part 5: Models' Comparison\n","- **Objective**: Evaluate and contrast the performance of different models.\n","- **Activities**: Address two open-ended questions.\n","\n","\n","## Your Implementation\n","\n","Please create a local copy of this template Colab's Notebook:\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FfXDvRMALIsd-IzPdf_Fn92OLVHjSY9I#scrollTo=JCFoQxO0H0yk)\n","\n","The assignment's instructions are there; follow the notebook.\n","\n","## Submission\n","- **Notebook Link**: Add the URL to your assignment's notebook in the `notebook_link.txt` file, following the format provided in the example.\n","- **Access**: Ensure the link has edit permissions enabled to allow modifications if needed.\n","- **Deadline**: <font color='green'>21/05/2024</font>.\n","- **Platform**: Continue using GitHub for submissions. Push your project to the team repository and monitor the test results under the actions section.\n","\n","Good Luck ðŸ¤—\n"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"8iWKz8IfKi5s","executionInfo":{"status":"ok","timestamp":1716305323668,"user_tz":-180,"elapsed":5354,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["# Prerequisite: Install the conllutils library before proceeding with the tasks\n","!pip install --q conllutils"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"iRm7zcfq56HF","executionInfo":{"status":"ok","timestamp":1716305323668,"user_tz":-180,"elapsed":8,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["# Standard Library Imports\n","import os\n","import random\n","import operator\n","import json\n","from typing import List, Tuple, Dict\n","from collections import defaultdict\n","\n","# Data Handling and Numerical Libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tabulate import tabulate\n","from google.colab import files\n","\n","# Machine Learning and Evaluation Libraries\n","from sklearn.metrics import classification_report, precision_recall_fscore_support\n","\n","# Natural Language Processing Libraries\n","import nltk\n","from nltk.tag import tnt\n","from nltk.metrics import ConfusionMatrix, precision, recall, f_measure\n","\n","# Deep Learning Libraries (PyTorch)\n","import torch as th\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# CoNLL Utilities for Data Handling\n","import conllutils\n","import gensim\n"]},{"cell_type":"markdown","metadata":{"id":"UH-Xvqip6Teu"},"source":["# Part 1 - Dataset Preparation\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iTv6rMt0oIw9"},"source":["For each package you use, set the random seed to 42."]},{"cell_type":"code","execution_count":76,"metadata":{"id":"PtM4HY2LoYDn","executionInfo":{"status":"ok","timestamp":1716305323668,"user_tz":-180,"elapsed":7,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["SEED = 42\n","# Set the random seed for Python\n","random.seed(SEED)\n","\n","# Set the random seed for numpy\n","np.random.seed(SEED)\n","\n","# Set the random seed for pytorch\n","th.manual_seed(SEED)\n","\n","# If using CUDA (for GPU operations)\n","th.cuda.manual_seed(SEED)"]},{"cell_type":"markdown","metadata":{"id":"kuvbl0hooXXx"},"source":["## Instructions\n","\n","### Step 1: Access the Dataset\n","The GUM dataset, which is part of the English corpora under Universal Dependencies, is specifically curated for academic and research purposes. You can download the dataset directly from the following GitHub repository:\n","\n","[UD English-GUM](https://github.com/UniversalDependencies/UD_English-GUM)"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"nsZsyTVC6Sw0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716305323668,"user_tz":-180,"elapsed":7,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}},"outputId":"914ca128-adfa-4d29-a1e6-451d8a9eceb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'UD_English-GUM' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/UniversalDependencies/UD_English-GUM"]},{"cell_type":"markdown","metadata":{"id":"airorSOzSRm_"},"source":["### Step 2: Reading the Data\n","We will use the (train/dev/test) files:\n","\n","\n","```\n","UD_English-GUM/en_gum-ud-train.conllu\n","UD_English-GUM/en_gum-ud-dev.conllu\n","UD_English-GUM/en_gum-ud-test.conllu\n","```\n","\n","## CoNLL-U Format\n","They are all formatted in the CoNLL-U format. You may read about it [here](https://universaldependencies.org/format.html). There is a utility library **conllutils**, which can help you read the data into the memory. It has already been installed and imported above.\n","\n","## Task: Create a Read Data Function\n","\n","### Function Specification\n","Create a function named `read_data` that:\n","- Takes a file path to a `.conllu` file as input.\n","- Returns a list of lists, where each inner list represents a sentence.\n","- Each sentence is composed of tuples containing the word ('form') and its corresponding Universal POS tag ('upos').\n","\n","The word is located in the column named 'form' and the POS tag in the column named 'upos' of the CoNLL-U format."]},{"cell_type":"code","execution_count":78,"metadata":{"id":"lkepUJYENXPq","executionInfo":{"status":"ok","timestamp":1716305323668,"user_tz":-180,"elapsed":5,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["DataType = list[list[tuple[str, str]]]"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"BYt7poZ3V2LV","executionInfo":{"status":"ok","timestamp":1716305323668,"user_tz":-180,"elapsed":5,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["def read_data(filepath: str) -> DataType:\n","    \"\"\"\n","    Reads a CoNLL-U formatted file and extracts sentences as lists of (word, POS tag) tuples.\n","\n","    Args:\n","    filepath (str): The path to the .conllu file to be read.\n","\n","    Returns:\n","    List[List[Tuple[str, str]]]: A list of sentences, where each sentence is a list of tuples containing the word and its POS tag.\n","    \"\"\"\n","    output = []\n","    # TO DO ----------------------------------------------------------------------\n","    current_sentence = [] # List to represent a sentence\n","\n","    # Open the path\n","    with open(filepath, 'r', encoding='utf-8') as file:\n","        # Iterate every line in flie\n","        for line in file:\n","            # print(f'line:\\n{line}')\n","            # Remove spaces in the beggining and the end of the sentence\n","            line = line.strip()\n","            if line:\n","                # Skip comment lines\n","                if line.startswith('#'):\n","                    continue\n","                else:\n","                    # Split to columns according to tabs\n","                    parts = line.split('\\t')\n","\n","                    # Ensure there are enough columns\n","                    if len(parts) > 3:\n","\n","                        # 'form' is typically in the second column\n","                        form = parts[1]\n","\n","                        # 'upos' is typically in the fourth column\n","                        upos = parts[3]\n","                        if upos != '_':\n","                          # Append a tuple of the word and POS tag\n","                          current_sentence.append((form, upos))\n","            else:\n","            # Empty line markes sentence boundaries - append the list to output and clear the sentence list\n","                if current_sentence:\n","                    output.append(current_sentence)\n","                    current_sentence = []\n","\n","        # Append the last sentence if the file doesn't end with a newline\n","        if current_sentence:\n","            output.append(current_sentence)\n","    # TO DO ----------------------------------------------------------------------\n","    return output"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"XmRpi7gXknQC","executionInfo":{"status":"ok","timestamp":1716305324320,"user_tz":-180,"elapsed":657,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["# Run this block once `read_data` is implemented.\n","train_dataset = read_data(\"UD_English-GUM/en_gum-ud-train.conllu\")\n","dev_dataset = read_data(\"UD_English-GUM/en_gum-ud-dev.conllu\")\n","test_dataset = read_data(\"UD_English-GUM/en_gum-ud-test.conllu\")"]},{"cell_type":"markdown","metadata":{"id":"h2E18JUi2YeD"},"source":["## Task: Create a Vocabulary Generation Function\n","\n","### Function Specification\n","Create a function named `generate_vocabs` for mapping words and tags into unique numbers so that we can use them in the tagging algorithms you will implement below. The function:\n","- Takes a list of sentences as input, where each sentence is composed of tuples containing a word and its corresponding Universal POS tag.\n","- Returns a tuple of two dictionaries:\n","  - The first dictionary maps each unique word to a unique integer.\n","  - The second dictionary maps each unique POS tag to a unique integer.\n","\n","Each word and POS tag should be mapped starting from 0, with each new word or tag encountered receiving the next sequential integer. This function is essential for converting textual data into a numerical format that can be used by tagging algorithms.\n","\n"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"j47ImFVyyoO4","executionInfo":{"status":"ok","timestamp":1716305324320,"user_tz":-180,"elapsed":8,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["def generate_vocabs(datasets: list[DataType]) -> tuple[dict[str, int], dict[str, int]]:\n","  \"\"\"\n","  Generates vocabularies mapping words and tags to unique indices from a list of datasets.\n","\n","  Args:\n","  datasets (list): A list of datasets where each dataset contains sentences formatted as lists of (word, tag) tuples.\n","\n","  Returns:\n","  Tuple[dict[str, int], dict[str, int]]: A tuple of two dictionaries:\n","      - The first dictionary maps each unique word to a unique integer.\n","      - The second dictionary maps each unique POS tag to a unique integer.\n","      Each dictionary includes a special '<<UNK>>' entry mapped to 0 to handle unknown words or tags.\n","  \"\"\"\n","  words_vocab = {\"<<UNK>>\": 0}\n","  tags_vocab = {\"<<UNK>>\": 0}\n","  word_index = 1\n","  tag_index = 1\n","  # TO DO ----------------------------------------------------------------------\n","  # Iterate over every dataset\n","  for dataset in datasets:\n","      # Iterate over each sentence, which is a list of (word, tag) tuples\n","      for sentence in dataset:\n","          # Iterate over each (word, tag) tuple in the sentence\n","          for word, tag in sentence:\n","              # Check if the word is not already in the words vocabulary\n","              if word not in words_vocab:\n","                  words_vocab[word] = word_index  # Map the word to the current word index\n","                  word_index += 1  # Increment the word index for the next unique word\n","              # Check if the tag is not already in the tags vocabulary\n","              if tag not in tags_vocab:\n","                  tags_vocab[tag] = tag_index  # Map the tag to the current tag index\n","                  tag_index += 1  # Increment the tag index for the next unique tag\n","\n","  # TO DO ----------------------------------------------------------------------\n","  return words_vocab, tags_vocab\n"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"FwzutNN21Z6m","executionInfo":{"status":"ok","timestamp":1716305324320,"user_tz":-180,"elapsed":8,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["# Run this block once `generate_vocabs` is implemented.\n","words_vocab, tags_vocab = generate_vocabs([train_dataset, dev_dataset, test_dataset])\n","reversed_words_vocab = {v: k for k, v in words_vocab.items()}\n","reversed_tags_vocab = {v: k for k, v in tags_vocab.items()}"]},{"cell_type":"markdown","metadata":{"id":"etK9iZIq8i0X"},"source":["# Part 2 - HMM Tagger\n","\n","### Task Description\n","Implement a class `HMMTagger` to perform POS tagging using a Hidden Markov Model (HMM).\n","\n","### Class Methods\n","- `fit`: This method should compute the transition probabilities matrix (A), emission probabilities matrix (B), and initial state probabilities vector (Pi) based on the training data. These matrices should reflect probabilities of transitions between tags, emissions of words given tags, and initial tag probabilities, respectively.\n","- `inference`: Implement this method to predict the best tag sequence for a given input sentence using the Viterbi decoding algorithm. The Viterbi algorithm is provided below.\n","### Additional Guidance\n","1. **Use of Vocabularies**: Utilize the vocabularies generated in Part 1. These should include a special entry for unknown words and tags (`<<UNK>>` at index 0). The indices in your vocabularies will correspond to the rows and columns of your A, B, and Pi matrices (np.array).\n","2. **Smoothing**: Apply Add-One Smoothing to all probability calculations to avoid zero probabilities. This technique adjusts the frequency counts for each observed event by adding one to each count.\n","3. **Word Conversion**: During inference, convert each word of the input sentence into its corresponding index using the word vocabulary. If a word is not found, use the index for `<<UNK>>`.\n","\n","### Implementation Tips\n","- You can use the vocab dictionaries directly, no need to pass them as a parameter to the functions.\n","- You may add functions to `HMMTagger` as needed.\n","- Ensure that your A, B, and Pi matrices handle unseen words/tags gracefully using the `<<UNK>>` index.\n"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"TpH7GuiQ9L6W","executionInfo":{"status":"ok","timestamp":1716305324320,"user_tz":-180,"elapsed":8,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["class HMMTagger:\n","  def __init__(self):\n","      \"\"\"\n","      Initializes the HMMTagger class with necessary attributes.\n","      \"\"\"\n","      self._Ï€ = np.zeros(len(tags_vocab))  # Initial state probabilities\n","      self._A = np.zeros((len(tags_vocab), len(tags_vocab)))  # Transition probabilities\n","      self._B = np.zeros((len(tags_vocab), len(words_vocab)))  # Emission probabilities\n","\n","  def fit(self, dataset: DataType):\n","      \"\"\"\n","      Trains the HMM model on the provided dataset.\n","\n","      Args:\n","          dataset (list): The training dataset containing sentences as lists of (word, tag) tuples.\n","      \"\"\"\n","      # TO DO ----------------------------------------------------------------------\n","      # Initialize matrices to count transitions, emissions, and initial states.\n","      transition_counts = np.zeros((len(tags_vocab), len(tags_vocab)))\n","      emission_counts = np.zeros((len(tags_vocab), len(words_vocab)))\n","      initial_counts = np.zeros(len(tags_vocab))\n","      tag_counts = np.zeros(len(tags_vocab)) # For the transition matrix\n","\n","      # Count occurrences\n","      for sentence in dataset:\n","          if sentence:  # Ensure the sentence is not empty.\n","              first_word, first_tag = sentence[0]  # Get the first word and its tag.\n","              initial_counts[tags_vocab.get(first_tag, 0)] += 1  # Increment the count for initial state.\n","              previous_tag_index = None  # Initialize previous_tag_index for tracking transitions.\n","\n","              # Iterate every word in the sentence\n","              for word, tag in sentence:\n","                  tag_index = tags_vocab.get(tag, 0)  # Get the index of the current tag.\n","                  word_index = words_vocab.get(word, 0)  # Get the index of the current word.\n","\n","                  # If there is a previous tag, update the transition matrix.\n","                  if previous_tag_index is not None:\n","                      transition_counts[previous_tag_index, tag_index] += 1\n","\n","                  previous_tag_index = tag_index  # Update previous_tag_index to the current tag for the next iteration.\n","\n","                  # Increment the count for the emission of the current word by the current tag.\n","                  emission_counts[tag_index, word_index] += 1\n","\n","                  # Do not increment tag counts for the last words of a sentence\n","                  if sentence.index((word, tag)) < len(sentence) - 1:\n","                      tag_counts[tag_index] += 1  # Increment the total count of the current tag (for transition matrix).\n","\n","\n","      # Calculate initial state probabilities include smoothing.\n","      # self._Ï€ = (initial_counts + 1) / (len(dataset) + len(tags_vocab))\n","\n","      # # Calculate transition probabilities include smoothing.\n","      # self._A = (transition_counts + 1) / (tag_counts[:, None] + len(tags_vocab))\n","\n","      # # Calculate emission probabilities include smoothing.\n","      # self._B = (emission_counts + 1) / (np.sum(emission_counts, axis=1)[:, None] + len(words_vocab)) # sum each row of emission to counts the total count of words emitted by tag (includign the last word - unlike the transition matrix)\n","\n","      # smoothing factor\n","      smoothing = 0.5\n","\n","      # Calculate initial state probabilities include smoothing.\n","      self._Ï€ = (initial_counts + smoothing) / (len(dataset) + smoothing*len(tags_vocab))\n","\n","      # Calculate transition probabilities include smoothing.\n","      self._A = (transition_counts + smoothing) / (tag_counts[:, None] + smoothing*len(tags_vocab))\n","\n","      # Calculate emission probabilities include smoothing.\n","      self._B = (emission_counts + smoothing) / (np.sum(emission_counts, axis=1)[:, None] + smoothing*len(words_vocab)) # sum each row of emission to counts the total count of words emitted by tag (includign the last word - unlike the transition matrix)\n","\n","       # TO DO ----------------------------------------------------------------------\n","\n","  def inference(self, sentence: list) -> list[Tuple[str, str]]:\n","      \"\"\"\n","      Predicts the best tag sequence for a given input sentence using the Viterbi decoding algorithm.\n","\n","      Args:\n","          sentence (list): The sentence to tag, as a list of words.\n","\n","      Returns:\n","          List[Tuple[str, str]]: Each word in the input sentence paired with its predicted tag.\n","      \"\"\"\n","      # TO DO ----------------------------------------------------------------------\n","      # Convert words in the sentence to indices using the words_vocab dictionary.\n","      # Words not found in the vocabulary are mapped to the index for '<<UNK>>'.\n","      word_indices = [words_vocab.get(word, words_vocab['<<UNK>>']) for word in sentence]\n","\n","      # Use the Viterbi algorithm to find the best sequence of tag indices.\n","      # This function assumes that viterbi is correctly implemented and available in the scope.\n","      best_tag_indices = viterbi(word_indices, self._A, self._B, self._Ï€)\n","\n","      # Convert tag indices back to tag strings.\n","      # Reverse the tags_vocab dictionary to map indices to tags.\n","      index_to_tag = {index: tag for tag, index in tags_vocab.items()}\n","\n","      # Pair each word with its corresponding best tag.\n","      tagged_sentence = [(word, index_to_tag[tag_index]) for word, tag_index in zip(sentence, best_tag_indices)]\n","\n","      return tagged_sentence\n","       # TO DO ----------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"Ni-3uwVmXiKB"},"source":["### Optional Task: Implement the Viterbi Algorithm\n","Implement the `viterbi` function to perform POS tagging using the Viterbi decoding algorithm. This algorithm finds the most probable sequence of hidden states (POS tags in this case) given a sequence of observations (words in the sentence).\n","\n","### Instructions\n","- **Pre-Implemented Code**: We provide a pre-implemented version of the Viterbi algorithm for your convenience. This implementation is fully functional and can be used directly in your HMM tagger.\n","  \n","- **Implementation Challenge**: Although a pre-implemented version is available, we encourage you to implement the Viterbi algorithm yourself. Doing so will help you understand the dynamics of dynamic programming in the context of POS tagging. Follow the pseudocode provided in the lecture slides to develop your own version of the algorithm.\n","\n","### Steps for Implementation\n","1. **Understand the Pseudocode**: Review the pseudocode provided in the slides from the class. Ensure you understand each step of the algorithm, including how the probabilities are updated and the backtracking process to recover the state sequence.\n","  \n","2. **Implement the Function**: Using the pseudocode as a guide, write your own `viterbi` function. Consider the matrices (A, B, and Pi) you prepared in the HMMTagger class as inputs along with the sequence of observations (word indices for a sentence).\n","  \n","3. **Test Your Implementation**: After implementing the function, test it with known inputs to ensure it produces the correct sequence of tags. Compare the results with those obtained from the pre-implemented version to validate your implementation.\n","\n","### Additional Tips\n","- **Handle Edge Cases**: Consider edge cases such as very short sentences, sentences containing many unknown words, and varying sentence structures.\n","- **Optimization**: Once your basic implementation is correct, think about potential optimizations to improve the efficiency of your code, especially if you are processing large datasets.\n"]},{"cell_type":"code","execution_count":84,"metadata":{"id":"DR6KJW2F9yqt","executionInfo":{"status":"ok","timestamp":1716305324320,"user_tz":-180,"elapsed":7,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["def viterbi(word_list: list, A: np.ndarray, B: np.ndarray, Pi: np.ndarray):\n","    \"\"\"\n","    Executes the Viterbi algorithm to find the most likely state sequence given a sequence of observations.\n","\n","    The Viterbi algorithm is a dynamic programming algorithm used to compute the most likely sequence of hidden states\n","    (called the Viterbi path) that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM).\n","\n","    Args:\n","        word_list (list): A list of indices corresponding to observed words.\n","        A (numpy.ndarray): The state transition probability matrix of shape (num_states, num_states) where A[i][j] is the probability of transitioning from state i to state j.\n","        B (numpy.ndarray): The emission probability matrix of shape (num_states, num_vocabulary) where B[i][j] is the probability of emitting symbol j from state i.\n","        Pi (numpy.ndarray): The initial state probability vector of length num_states where Pi[i] is the probability of starting in state i.\n","\n","    Returns:\n","        list: The most likely sequence of states (as indices) for the given sequence of observations.\n","\n","    The function uses three main steps: initialization, recursion, and termination:\n","    - **Initialization**: Set the initial probabilities of being in each state.\n","    - **Recursion**: For each subsequent observation, compute probabilities of each state based on the observation and transition probabilities from previous states.\n","    - **Termination**: Backtrace to determine the most probable path through the state space.\n","    \"\"\"\n","    # Number of states\n","    num_states = len(A)\n","    # Length of the observed sequence\n","    T = len(word_list)\n","\n","    # Create the path probability matrix V\n","    V = [[0 for _ in range(num_states)] for _ in range(T)]\n","    # Create a path backpointer matrix to store the argmax indices\n","    path = [[0 for _ in range(num_states)] for _ in range(T)]\n","\n","    # Initialization step\n","    for s in range(num_states):\n","        V[0][s] = Pi[s] * B[s][word_list[0]]\n","        path[0][s] = 0\n","\n","    # Recursion step\n","    for t in range(1, T):\n","        for s in range(num_states):\n","            # Find the state with the maximum probability to reach state s\n","            max_tr_prob = V[t-1][0] * A[0][s]\n","            prev_state_selected = 0\n","            for prev_state in range(1, num_states):\n","                tr_prob = V[t-1][prev_state] * A[prev_state][s]\n","                if tr_prob > max_tr_prob:\n","                    max_tr_prob = tr_prob\n","                    prev_state_selected = prev_state\n","            # Multiply the max probability by the probability of observing the symbol at state s\n","            max_prob = max_tr_prob * B[s][word_list[t]]\n","            V[t][s] = max_prob\n","            path[t][s] = prev_state_selected\n","\n","    # Termination step\n","    # Find the best path by looking for the maximum probability in the last column\n","    opt = []\n","    max_prob = -1\n","    best_last_state = 0\n","    for s in range(num_states):\n","        if V[T-1][s] > max_prob:\n","            max_prob = V[T-1][s]\n","            best_last_state = s\n","    opt.append(best_last_state)\n","\n","    # Follow the back pointers to decode the best path\n","    previous = best_last_state\n","    for t in range(T-1, 0, -1):\n","        opt.insert(0, path[t][previous])\n","        previous = path[t][previous]\n","\n","    return opt"]},{"cell_type":"code","execution_count":85,"metadata":{"id":"Bkj25vm2knij","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716305324320,"user_tz":-180,"elapsed":7,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}},"outputId":"b6002173-bc5f-4ed0-88fb-aa702bb4ba6e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 1, 1, 1]"]},"metadata":{},"execution_count":85}],"source":["# Example for using Viterbi algorithm (from the course slides)\n","A = np.array([[0.3, 0.7], [0.2, 0.8]])\n","B = np.array([[0.1, 0.1, 0.3, 0.5], [0.3, 0.3, 0.2, 0.2]])\n","Pi = np.array([0.4, 0.6])\n","\n","viterbi([0, 3, 2, 0], A, B, Pi)"]},{"cell_type":"markdown","metadata":{"id":"cnKUWMhyUgXy"},"source":["## Train & Evaluate\n","\n","## Task: Implement the Train and Evaluate Tagger Function\n","\n","### Function Specification\n","Create a function named `train_evaluate_tagger` that combines the training and evaluation processes for a tagging algorithm. This integrated function should:\n","- **Input**:\n","  - **tagger** (`HMMTagger` or `EmbeddingsTagger`): The POS tagger to be trained and evaluated.\n","  - **train_dataset** (`List[List[Tuple[str, str]]]`): The dataset on which the tagger will be trained. Training is executed only if the `train` flag is set to True.\n","  - **eval_dataset** (`List[List[Tuple[str, str]]]`): The dataset used for evaluating the tagger's performance.\n","  - **train** (`bool`): A boolean flag indicating whether the training phase should be executed. If set to True, the `fit` method of the tagger will be called before evaluation.\n","- **Functionality**:\n","  - Train the tagger using the `train_dataset`.\n","  - Evaluate the trained tagger on the `test_dataset` to compute performance metrics.\n","  - Return the following evaluation metrics: accuracy, precision, recall, and F1-score.\n","- **Outputs**: A tuple consisting of three elements: precision, recall, F1-score, as returned by the `precision_recall_fscore_support` function from the `sklearn.metrics` module, when called with the parameter `average=\"macro\"`.\n","* Note the the support metric is missed (Why?).\n","\n","\n","### Evaluation Metrics\n","Upon execution, the `train_evaluate_tagger` function provides the following metrics:\n","- **Macro-average Precision, Recall, and F1 Score**: These scores are calculated over all tags to assess the overall effectiveness of the tagger in recognizing the correct tags across different types of words.\n","- **Performance Breakdown by Tag**: Detailed metrics for each tag, helping to identify which tags are most and least accurately predicted.\n","- **Overall Word-Level Accuracy**: Measures the percentage of words correctly tagged by the tagger across the entire evaluation dataset.\n","\n"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"Vumzm2lB7p1-","executionInfo":{"status":"ok","timestamp":1716305324321,"user_tz":-180,"elapsed":7,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["def train_evaluate_tagger(tagger, train_dataset: DataType=train_dataset, eval_dataset: DataType=test_dataset, train=True):\n","    \"\"\"\n","    Trains (optional) and evaluates a POS tagger, returning performance metrics.\n","\n","    This function trains the given tagger if specified, and evaluates it on a provided dataset.\n","    It calculates and returns the macro-average precision, recall, and F1-score.\n","\n","    Args:\n","        tagger (HMMTagger or EmbeddingsTagger): The POS tagger to be trained and evaluated.\n","        train_dataset (List[List[Tuple[str, str]]]): The dataset to train the tagger.\n","        eval_dataset (List[List[Tuple[str, str]]]): The dataset to evaluate the tagger.\n","        train (bool): A flag indicating whether the tagger should be trained.\n","\n","    Returns:\n","        tuple: A tuple containing average precision, recall, and F1-score.\n","\n","    The function also prints a classification report for detailed performance analysis.\n","    \"\"\"\n","\n","    true_tags = []\n","    predicted_tags = []\n","\n","    # TO DO ----------------------------------------------------------------------\n","\n","    if train:\n","        # Training the tagger with the provided training dataset.\n","        tagger.fit(train_dataset)\n","\n","    # Evaluating the tagger on the evaluation dataset\n","    for sentence in eval_dataset:\n","        words, tags = zip(*sentence)  # Splitting words and tags\n","        words, tags = zip(*sentence)  # Splitting words and tags\n","        true_tags.extend(tags)  # Collecting all true tags\n","\n","        # Using the tagger to predict tags based on words\n","        predicted_sentence_tags = tagger.inference(words)\n","        _, predicted_sentence_tags = zip(*predicted_sentence_tags)\n","        predicted_tags.extend(predicted_sentence_tags)  # Collecting all predicted tags\n","\n","    # TO DO ----------------------------------------------------------------------\n","\n","    # Compute precision, recall, F1-score, and support for each class\n","    precision, recall, f1_score, support = precision_recall_fscore_support(true_tags, predicted_tags, average=None)\n","\n","    # Display classification report\n","    report = classification_report(true_tags, predicted_tags)\n","    print(\"Classification Report:\")\n","    print(report)\n","\n","    # Return the macro-average metrics\n","    avg_precision, avg_recall, avg_f1, _ = precision_recall_fscore_support(true_tags, predicted_tags, average='macro')\n","\n","    return avg_precision, avg_recall, avg_f1\n"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"l0uM2tozQmpq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716305328305,"user_tz":-180,"elapsed":3991,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}},"outputId":"60591846-b8ec-45b2-9144-c854bfe1871c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","     <<UNK>>       0.00      0.00      0.00         0\n","         ADJ       0.82      0.80      0.81      1622\n","         ADP       0.88      0.97      0.92      2481\n","         ADV       0.85      0.78      0.82      1114\n","         AUX       0.81      0.97      0.88      1189\n","       CCONJ       0.96      0.97      0.97       839\n","         DET       0.86      0.97      0.91      2111\n","        INTJ       0.67      0.69      0.68       163\n","        NOUN       0.88      0.83      0.85      4239\n","         NUM       0.78      0.70      0.74       440\n","        PART       0.82      0.81      0.82       519\n","        PRON       0.83      0.98      0.90      1746\n","       PROPN       0.82      0.56      0.67      1628\n","       PUNCT       0.93      1.00      0.96      3027\n","       SCONJ       0.78      0.56      0.65       340\n","         SYM       0.58      0.40      0.47        35\n","        VERB       0.88      0.79      0.83      2480\n","           X       0.56      0.16      0.24        32\n","\n","    accuracy                           0.86     24005\n","   macro avg       0.76      0.72      0.73     24005\n","weighted avg       0.86      0.86      0.86     24005\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# Train (optional) and Evaluate your HMMTagger here\n","hmmTagger = HMMTagger()\n","precision_hmm, recall_hmm, f1_hmm = train_evaluate_tagger(hmmTagger)"]},{"cell_type":"markdown","metadata":{"id":"D3fPdEWBetDE"},"source":["## Part 3 - POS Tagging with Pre-trained Word Embeddings\n","\n","### Task Overview\n","Develop a feed-forward neural network for Part-of-Speech (POS) tagging using the PyTorch framework. This tagger leverages pre-trained word embeddings from the word2vec Google News dataset, enhancing the semantic understanding of words compared to traditional tagging methods.\n","\n","### Task 1: Initialization of Embeddings (`init_embeddings`)\n","Create an `init_embeddings` method within the `EmbeddingsTagger` class to load and set up pre-trained word embeddings:\n","- Load the Google News word vectors from a specified file path.\n","- Initialize a matrix to hold these embeddings where each word in your vocabulary is represented by a vector. For words not in the pre-trained model, initialize their vectors randomly.\n","-  <font color='red'>**NOTE:** Before you start implementing the init_embeddings method, make sure to create your own copy of the pre-trained embeddings (.bin file) in your Google Drive from [the following Google Drive folder](https://drive.google.com/drive/folders/1-HcSBfqaX0PCFiT8TsiYjFZRQJRm2R5v?usp=sharing). You will need to use the gensim library to load this file.</font>\n","\n","### Task 2: Implementation of `EmbeddingsTagger` Class\n","Extend PyTorch's `nn.Module` to implement the `EmbeddingsTagger` class. This class should utilize the embeddings matrix and include:\n","- An embedding layer that is initialized with the pre-trained embeddings.\n","- A linear layer to combine embeddings of the current and previous words with a one-hot encoded previous tag to predict the current tag.\n","- A `forward` method that outlines the data flow through the network.\n","\n","### Task 3: Model Training and Evaluation (`fit` and `inference` Methods)\n","Implement training and inference functionalities within the `EmbeddingsTagger` class:\n","- **Train Method (`fit`)**: Set up the model training using the specified training dataset. This includes iterating through the dataset, applying the model to predict tags, and updating model parameters based on the loss computation.\n","- **Inference Method (`inference`)**: Configure the model to predict tags on a new dataset, assessing the model's effectiveness on unseen data.\n","\n","### Model Performance Monitoring\n","Utilize the `train_evaluate_tagger` function to oversee the training process and evaluate the model:\n","- Configure this function to handle model training with appropriate optimizer and loss settings.\n","- Monitor and report the model's performance metrics during training and on a test dataset to ensure effective tagging.\n"]},{"cell_type":"markdown","metadata":{"id":"lOGqtQtyUsA9"},"source":["### Google News pre-trained embeddings\n"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3859,"status":"ok","timestamp":1716305332160,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"},"user_tz":-180},"id":"XROdy8sHZ-Qu","outputId":"a3a5c160-c86b-45a2-dcc6-055720d0e4d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":89,"metadata":{"id":"Ljrof4K3Va70","executionInfo":{"status":"ok","timestamp":1716305332160,"user_tz":-180,"elapsed":4,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["class EmbeddingsTagger(nn.Module):\n","    def __init__(self, device=\"cpu\"):\n","        \"\"\"\n","        Initializes an embeddings-based POS tagger that uses word embeddings from a pre-trained model.\n","\n","        Args:\n","            device (str): The device (cpu or cuda) the model should operate on for tensor operations.\n","        \"\"\"\n","        self.device = device\n","        super(EmbeddingsTagger, self).__init__()\n","        self.embedding_dim = 300  # Dimension of Google News embeddings\n","        self.tagset_size = len(tags_vocab)\n","        self.word_embeddings = self.init_embeddings()\n","        self.to(device)\n","\n","\n","        # The input dimension to the linear layer is twice the embedding_dim (for two words)\n","        # plus tagset_size (for the one-hot encoded previous tag)\n","        self.linear_layer = nn.Linear(2 * self.embedding_dim + self.tagset_size, self.tagset_size).to(self.device)\n","\n","\n","    def init_embeddings(self) -> nn.Embedding:\n","        \"\"\"\n","        Loads word embeddings from a pre-trained Google News model and initializes an embedding layer.\n","\n","        Returns:\n","            nn.Embedding: A PyTorch embedding layer with the pre-trained word embeddings.\n","        \"\"\"\n","        # Specify the path to the Google News model in your Google Drive\n","        path_to_google_news_vectors = '/content/drive/My Drive/Colab Notebooks/NLP/GoogleNews-vectors-negative300.bin'\n","\n","        # Load Google News Vectors\n","        model = gensim.models.KeyedVectors.load_word2vec_format(path_to_google_news_vectors, binary=True)\n","\n","        # Prepare a matrix to hold the embeddings\n","        embedding_matrix = np.zeros((len(words_vocab), 300))  # Ensure 'words_vocab' maps words to indices, 300 is the dimension of embeddings\n","\n","        # TO DO ----------------------------------------------------------------------\n","        # Iterate over all the words in the vocabulary\n","        for word, idx in words_vocab.items():\n","\n","            # Use the pre-trained vector if the word appears in the model\n","            if word in model:\n","                embedding_matrix[idx] = model[word]\n","\n","            # Random initialization if the word is not found\n","            else:\n","                embedding_matrix[idx] = np.random.normal(size=(model.vector_size,))\n","\n","        # Convert the numpy matrix to a torch tensor\n","        embedding_tensor = th.tensor(embedding_matrix, dtype=th.float32)\n","\n","        # Create an embedding layer in PyTorch\n","        embedding_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=False)\n","\n","        return embedding_layer\n","\n","        # TO DO ----------------------------------------------------------------------\n","\n","    def forward(self, current_word_index: int, previous_word_index: int, prev_tag_one_hot: th.Tensor):\n","        \"\"\"\n","        Forward pass of the tagger to compute logits for each tag.\n","\n","        Args:\n","            current_word_index (int): Index of the current word in the vocabulary.\n","            previous_word_index (int): Index of the previous word in the vocabulary.\n","            prev_tag_one_hot (th.tensor): One-hot encoded tensor of the previous tag.\n","\n","        Returns:\n","            torch.Tensor: Log probabilities for each tag.\n","        \"\"\"\n","        # TO DO ----------------------------------------------------------------------\n","\n","        # Embeddings for current and previous words\n","        current_word_embedding = self.word_embeddings(th.tensor([current_word_index], device=self.device))\n","        previous_word_embedding = self.word_embeddings(th.tensor([previous_word_index], device=self.device))\n","\n","        # Concatenate embeddings and previous tag one-hot vector\n","        # combined_input = th.cat((current_word_embedding, previous_word_embedding, prev_tag_one_hot.unsqueeze(0)), dim=1)\n","        combined_embedding = th.cat((th.cat((current_word_embedding, previous_word_embedding), dim=1).view(-1), prev_tag_one_hot), dim=0).to(self.device)\n","\n","        # Pass the concatenated input through the linear layer to produce logits\n","        tag_logits = self.linear_layer(combined_embedding)\n","\n","        tag_scores = nn.functional.log_softmax(tag_logits, dim=0)\n","\n","        return tag_scores\n","        # TO DO ----------------------------------------------------------------------\n","\n","    def fit(self, dataset=list(train_dataset), epochs=15):\n","        \"\"\"\n","        Trains the tagger on the provided dataset for a specified number of epochs.\n","\n","        Args:\n","            dataset (list): The dataset to train the model on.\n","            epochs (int): Number of training epochs.\n","        \"\"\"\n","        loss_function = nn.NLLLoss()\n","        optimizer = optim.Adam(self.parameters(), lr=0.01)\n","\n","        # preparing instances for training\n","        instances = []\n","\n","        # TO DO ----------------------------------------------------------------------\n","\n","        # Iterate over all the sentences in the dataset\n","        for sentence in dataset:\n","\n","          # Iterate over every (word,tag) in the current sentence\n","          for i in range(1, len(sentence)):\n","\n","            current_word = words_vocab.get(sentence[i][0], 0) # Current word\n","\n","            previous_word = words_vocab.get(sentence[i-1][0], 0)  # Previouse word\n","\n","            prev_tag = tags_vocab.get(sentence[i-1][1], 0)  # Previouse tag\n","\n","            target_tag = th.tensor(tags_vocab.get(sentence[i][1], 0)).to(self.device) # Current tag\n","\n","            prev_tag_one_hot = nn.functional.one_hot(th.tensor(prev_tag), num_classes=self.tagset_size).to(self.device) # One hot vector according to the previouse tag\n","\n","            instances.append((current_word, previous_word, prev_tag_one_hot, target_tag)) # Add a 4-tuple to instances\n","        # TO DO ----------------------------------------------------------------------\n","\n","        loss_c = 0\n","        for epoch in range(epochs):\n","          for index, (current_word, previous_word, prev_tag_one_hot, target_tag) in enumerate(instances):\n","            self.zero_grad()\n","            tag_scores = self(current_word, previous_word, prev_tag_one_hot) # forward\n","            loss = loss_function(tag_scores, target_tag)\n","            loss.backward()\n","            optimizer.step()\n","            loss_c += loss.item()\n","            # Add printing (logging) as you wish (loss, epochs, etc.)\n","          print(f'Epoch: {epoch+1}/{epochs}; avg loss: {loss_c/(len(instances)*(epoch+1))}')\n","\n","\n","    def inference(self, sentence: list) -> List[Tuple[str, str]]:\n","        \"\"\"\n","        Predicts the tags for each word in a given sentence.\n","\n","        Args:\n","            sentence (list): The sentence to tag, given as a list of words.\n","\n","        Returns:\n","            list: A list of tuples containing each word and its predicted tag.\n","        \"\"\"\n","\n","        # TO DO ----------------------------------------------------------------------\n","        predicted_tags = []\n","\n","        # initilaize first word and tag as unknown\n","        prev_word = \"<<UNK>>\"\n","        prev_tag = \"<<UNK>>\"\n","\n","        # Iterate the (word, tag) in the sentence\n","        for word in sentence:\n","\n","          curr_word_index = words_vocab.get(word, 0)  # Index of the currewnt word\n","\n","          prev_word_index = words_vocab.get(prev_word, 0) # index of the previouse word\n","\n","          prev_tag_one_hot = nn.functional.one_hot(th.tensor(tags_vocab.get(prev_tag, 0)), num_classes=self.tagset_size).to(self.device) # One hot vector according to the previouse tag\n","\n","          net_output = self(curr_word_index, prev_word_index, prev_tag_one_hot) # Forward output (scores of softmax)\n","\n","          predicted_tag = th.argmax(net_output).item() # Predict the maximal score as the current word's tag (tag index)\n","\n","          predicted_tag = reversed_tags_vocab[predicted_tag] # Pull the tag name according to the index\n","\n","          predicted_tags.append((word, predicted_tag)) # Add the the porediction the list\n","\n","          prev_word = word\n","          prev_tag = predicted_tag\n","\n","        return predicted_tags\n","        # TO DO ----------------------------------------------------------------------\n"]},{"cell_type":"code","execution_count":90,"metadata":{"id":"l_IoQJ2PXOaZ","executionInfo":{"status":"ok","timestamp":1716305457763,"user_tz":-180,"elapsed":125607,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":["# Initialize the model with pre-trained embeddings\n","# device = th.device(\"cuda\")\n","device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n","embeddings_tagger = EmbeddingsTagger(device=device)"]},{"cell_type":"code","execution_count":91,"metadata":{"id":"8IDlGSTbd-CZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716310285266,"user_tz":-180,"elapsed":1134317,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}},"outputId":"87604efe-59cc-426a-a5c9-34f084e414bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/15; avg loss: 2.571086335515966\n","Epoch: 2/15; avg loss: 2.5630533204321706\n","Epoch: 3/15; avg loss: 2.564814402305833\n","Epoch: 4/15; avg loss: 2.5774308049606542\n","Epoch: 5/15; avg loss: 2.6008999434512536\n","Epoch: 6/15; avg loss: 2.6196568758508687\n","Epoch: 7/15; avg loss: 2.6313593245371973\n","Epoch: 8/15; avg loss: 2.6421198581991074\n","Epoch: 9/15; avg loss: 2.654961804984018\n","Epoch: 10/15; avg loss: 2.6637649025823364\n","Epoch: 11/15; avg loss: 2.673205413451371\n","Epoch: 12/15; avg loss: 2.682749160862143\n","Epoch: 13/15; avg loss: 2.6917902717445616\n","Epoch: 14/15; avg loss: 2.700468653394183\n","Epoch: 15/15; avg loss: 2.709436074443069\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","         ADJ       0.70      0.77      0.73      1622\n","         ADP       0.89      0.89      0.89      2481\n","         ADV       0.81      0.74      0.77      1114\n","         AUX       0.96      0.93      0.94      1189\n","       CCONJ       0.98      0.97      0.98       839\n","         DET       0.96      0.96      0.96      2111\n","        INTJ       0.93      0.34      0.50       163\n","        NOUN       0.78      0.85      0.81      4239\n","         NUM       0.89      0.80      0.84       440\n","        PART       0.81      0.86      0.83       519\n","        PRON       0.91      0.94      0.93      1746\n","       PROPN       0.82      0.54      0.65      1628\n","       PUNCT       0.99      0.99      0.99      3027\n","       SCONJ       0.53      0.54      0.53       340\n","         SYM       0.43      0.83      0.56        35\n","        VERB       0.79      0.85      0.82      2480\n","           X       0.37      0.31      0.34        32\n","\n","    accuracy                           0.85     24005\n","   macro avg       0.80      0.77      0.77     24005\n","weighted avg       0.86      0.85      0.85     24005\n","\n"]}],"source":["precision_embeddings, recall_embeddings, f1_embeddings = train_evaluate_tagger(embeddings_tagger)"]},{"cell_type":"code","source":["print(precision_embeddings, recall_embeddings, f1_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xK3oph-qJM_T","executionInfo":{"status":"ok","timestamp":1716310285266,"user_tz":-180,"elapsed":3,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}},"outputId":"13a6fb98-b424-4454-db07-d08303b3daba"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["0.7968547521192858 0.7700362584632802 0.7690032903980393\n"]}]},{"cell_type":"markdown","metadata":{"id":"-YZO0uGL-4S-"},"source":["\n","# Part 4 - NLTK Tagger\n","\n","### Overview\n","In this final part of the assignment, you will evaluate the performance of the HMM-based and feed-forward taggers you developed against a Maximum Entropy Markov Model (MEMM) tagger implemented using the Natural Language Toolkit (NLTK), a popular NLP library. Perform comparison should cover the test dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"Fl3oahPVpsBt"},"source":["#### Step 1: Training the MEMM Tagger\n"]},{"cell_type":"code","execution_count":93,"metadata":{"id":"CmkNPYzOvDdf","executionInfo":{"status":"ok","timestamp":1716310285892,"user_tz":-180,"elapsed":627,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}}},"outputs":[],"source":[" # TO DO ----------------------------------------------------------------------\n","MEMM_tagger = tnt.TnT()  # Assign a POS tagger of NLTK\n","\n","MEMM_tagger.train(train_dataset) # Train the tagger over tha train dataset\n"," # TO DO ----------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"NQ8YwpiyvJ6l"},"source":["#### Step 2: Evaluation\n","- Evaluate the trained MEMM tagger on  the test dataset.\n","- Calculate performance metrics such as accuracy, and F1-score. NLTK provides utilities that can help compute these metrics efficiently.\n","- Save & Print the eveluation scores.\n"]},{"cell_type":"code","execution_count":94,"metadata":{"id":"cnw1UbMj1n3f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716310382999,"user_tz":-180,"elapsed":97108,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}},"outputId":"f7f68766-b83e-4b95-f116-2367851ee115"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8636534055405124"]},"metadata":{},"execution_count":94}],"source":[" # TO DO ----------------------------------------------------------------------\n"," # Accurcy\n"," # Import accuracy method\n","from sklearn.metrics import accuracy_score\n","\n","# Tag the test dataset using the NLTK trained model\n","tagged_testdata = MEMM_tagger.tagdata([[word for word,tag in sentence] for sentence in test_dataset])\n","\n","# The true taggings\n","testdata_true_tags = [tag for sentence in test_dataset for word,tag in sentence]\n","\n","# Slice the tags out of the tagged dataset\n","testdata_predicted_tags = [tag for sentence in tagged_testdata for word,tag in sentence]\n","\n","accuracy_tnt_pos_tagger = accuracy_score(testdata_true_tags, testdata_predicted_tags)\n"," # TO DO ----------------------------------------------------------------------\n","accuracy_tnt_pos_tagger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-7FX8tW0y7k"},"outputs":[],"source":[" # TO DO ----------------------------------------------------------------------\n","confusion_matrix = ConfusionMatrix(testdata_true_tags, testdata_predicted_tags)\n","print(f\"Confusion Matrix:\\n{confusion_matrix}\")\n","\n","\n","print(f\"Accuracy: {accuracy_tnt_pos_tagger:.4f}\")\n","\n","f1_scores = f_measure(set(testdata_true_tags), set(testdata_predicted_tags))\n","print(f\"F1-score: {f1_scores:.4f}\")\n","\n","tags_recall = recall(set(testdata_true_tags), set(testdata_predicted_tags))\n","print(f\"Recall: {tags_recall}\")\n","\n"," # TO DO ----------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"TG4PmdJ_qRHi"},"source":["# Evaluatoin and Comparison\n","Compare the results obtained from the MEMM tagger with those from your HMM and feed-forward neural network taggers.\n","\n","This part won't be tested by the autograder.\n","\n","#### Discuss the following:\n"]},{"cell_type":"markdown","metadata":{"id":"fXpQoDsdqihY"},"source":["\n","* <font color='red'>(**?**)</font> Which tagger performed best on each dataset and why?\n","* <font color='green'>(**Answer**)</font> :\n","\n","**HMM:** The HMM showed moderate performance and was outperformed by the MEMM. HMMs typically perform well on simpler or well-represented sequential patterns but struggle with more complex or less frequent sequences. According to the results, it seems that the Universal Dependencies dataset has some complex sequences, but overall contains simple texts that can be learned by the HMM.\\\n","**Embedding Tagger:** This model achieved performance comparable to the HMM only after extensive training (5 epochs and more). This suggests that while capable of learning intricate patterns and dependencies given enough training time, it requires significant data exposure and computational resources to optimize its weights adequately. Nevertheless, after a massive training procedure (15 epochs), the model reached good performance, but still hasn't reached the accuracy of the MEMM. \\\n","**MEMM (NLTK)**: The MEMM tagger provided by NLTK demonstrated the best performance on the dataset. This superior performance can be attributed to its ability to use complex feature sets and handle dependencies more effectively than the other two models. By integrating diverse contextual information, MEMM can generalize well on unseen data (like the test dataset), making it robust and accurate for tagging tasks in varied contexts.\n"]},{"cell_type":"markdown","metadata":{"id":"-SgZs91PqyIG"},"source":["* <font color='red'>(**?**)</font> What are the strengths and weaknesses of each approach (HMM, feed-forward neural network, MEMM) in terms of training time, accuracy, and generalizability?\n","* <font color='green'>(**Answer**)</font> :\n","\n","**HMMs**\\\n","Strengths:\n","\n","1.Fast Training: HMMs are computationally efficient due to their reliance on statistical counts, which are \"low cost\" in computational aspects.\n","2. Predictions of Short and Simple Texts: HMMs are generally good for sequence prediction where the probability of a tag depends strongly on its immediate predecessor. Thus, they are likely to perform well (high accuracy) on simpler or smaller texts where dependency contexts are less diverse.\n","\n","Weaknesses:\n","\n","1. Complexed texts Handling: Only considers immediate previous states (first-order Markov assumption), which may not capture complex dependencies.\n","2. Lower Accuracy: Generally less accurate on more complex or contextually rich datasets\n","3. Low generalizability - Due to the limitation to simple texts and the fact that the model is based on statistical counts of the training dataset, HMMs might struggle to perform well on unseen texts (especially if they are complex).\n","\n","**Feed-Forward Neural Networks**\\\n","Strengths:\n","\n","1. Complexed texts: Capable of modeling complex and non-linear relationships which can lead to higher accuracy for texts with complex dependencies.\n","2. Generalizability: Learns feature representations of texts, therefore can handle better unseen texts/relationships.\n","\n","Weaknesses:\n","\n","1. Longer Training Times: Typically require more computational resources and time, especially as network depth and dataset size increase.\n","2. Accuracy:  In the given dataset of Universal Dependencies, it seems that to achieve accuracy as high as HMM, we need to run more than 10 epochs, and the MEMM stays higher. Thus, we infer that the MLP doesn't succeed in predicting the different relationships.\n","\n","**Maximum Entropy Markov Models (MEMMs) - NLTK**\\\n","Strengths:\n","\n","1. Flexible Feature Integration: Can incorporate a diverse set of contextual features and dependencies, potentially improving accuracy in texts with context beyond the immediate previous state (supports our results, where the MEMM accuracy is the highest).\n","2. Generalizability:  Maximum entropy is capable of capturing more complex patterns than the Markov property. Therefore, MEMM can evaluate complex texts with non-linear combinations of features compared to HMM and handle unseen types of texts.\n","\n","\n","Weaknesses:\n","\n","1. Label Bias Problem: In situations where a particular state has very few outgoing transitions (possible next states), the model can overly favor these transitions because there is limited competition for probability mass at this step, due to the normalization process in MEMM that tends to allocate a disproportionately large probability mass to these few transitions.\n","2. Training Time: While MEMMs can have faster training times than neural networks, they are often slower than HMMs, due to the fact that MEMMs' training process uses gradient descent to learn the model parameters while HMMs primarily rely on counting and straightforward probability calculations, which are faster than GD.\n"]},{"cell_type":"markdown","metadata":{"id":"_q9_TVEPUKkO"},"source":["# Testing\n","Copy the content of the **tests.py** file from the repo and paste below. This will create the results.json file and download it to your machine."]},{"cell_type":"code","execution_count":96,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"mJgUy9qNUE0l","executionInfo":{"status":"ok","timestamp":1716310383029,"user_tz":-180,"elapsed":1,"user":{"displayName":"Orr Zwebner","userId":"01408657010698171509"}},"outputId":"7badf232-8efd-418f-c181-18959ee68304"},"outputs":[{"data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":["download(\"download_8f81bf60-e9c1-4d56-a3d9-21898746b270\", \"results.json\", 526)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_8f81bf60-e9c1-4d56-a3d9-21898746b270\", \"results.json\", 526)"]},"metadata":{}}],"source":["####################\n","# PLACE TESTS HERE #\n","####################\n","def test_read_data_data_types():\n","    data = read_data(\"UD_English-GUM/en_gum-ud-train.conllu\")\n","    result = {\n","        'is_data_list': type(data) == list,\n","        'is_data_first_element_list': type(data[0]) == list,\n","        'is_data_first_element_first_item_tuple': type(data[0][0]) == tuple\n","    }\n","    return result\n","\n","def test_read_data_len_train_data():\n","    return {\n","        'train_data_length': len(read_data(\"UD_English-GUM/en_gum-ud-train.conllu\")),\n","    }\n","\n","def test_generate_vocabs():\n","    return {\n","        'vocab_size': len(words_vocab),\n","        'num_tags': len(tags_vocab)\n","    }\n","\n","def test_hmm():\n","    return {\n","        'precision': round(precision_hmm, 2),\n","        'recall': round(recall_hmm, 2),\n","        'f1': round(f1_hmm, 2),\n","    }\n","\n","def test_embeddings_model():\n","    return {\n","        'precision': round(precision_embeddings, 2),\n","        'recall': round(recall_embeddings, 2),\n","        'f1': round(f1_embeddings, 2),\n","    }\n","\n","\n","def test_nltk_tagger():\n","    return {\n","        'accuracy': round(accuracy_tnt_pos_tagger, 2)\n","    }\n","\n","TESTS = [test_read_data_data_types, test_read_data_len_train_data, test_generate_vocabs, test_hmm, test_embeddings_model, test_nltk_tagger]\n","\n","\n","# Run tests and save results\n","res = {}\n","for test in TESTS:\n","    try:\n","        cur_res = test()\n","        res.update({test.__name__: cur_res})\n","    except Exception as e:\n","        res.update({test.__name__: repr(e)})\n","\n","with open('results.json', 'w') as f:\n","    json.dump(res, f, indent=2)\n","\n","# Download the results.json file\n","files.download('results.json')\n","\n","####################"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[{"file_id":"15f7Zt0KcWNbebq_EuG-gL2swlQKhPFRi","timestamp":1715090609141},{"file_id":"1FfXDvRMALIsd-IzPdf_Fn92OLVHjSY9I","timestamp":1714916957945},{"file_id":"1tcx5XsFTIHKEFlcugc6GdpBNZdqRXMsY","timestamp":1649324772539}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
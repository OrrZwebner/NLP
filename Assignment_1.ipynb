{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FECp14-d_F2e"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za-DgcYB_IQx",
        "outputId": "87f4441a-743d-43a8-835d-30a87450fe39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'assignment_1'...\n",
            "remote: Enumerating objects: 150, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 150 (delta 35), reused 22 (delta 22), pack-reused 106\u001b[K\n",
            "Receiving objects: 100% (150/150), 6.79 MiB | 14.24 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NLP-Reichman/assignment_1.git\n",
        "!mv assignment_1/data data\n",
        "!rm assignment_1/ -r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i2bOXTB8Dvc"
      },
      "source": [
        "# Introduction\n",
        "In this assignment you will be creating tools for learning and testing language models. The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
        "The relevant files are under the data folder:\n",
        "\n",
        "- en.csv (or the equivalent JSON file)\n",
        "- es.csv (or the equivalent JSON file)\n",
        "- fr.csv (or the equivalent JSON file)\n",
        "- in.csv (or the equivalent JSON file)\n",
        "- it.csv (or the equivalent JSON file)\n",
        "- nl.csv (or the equivalent JSON file)\n",
        "- pt.csv (or the equivalent JSON file)\n",
        "- tl.csv (or the equivalent JSON file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u1qR7iaq_GU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHN0tWTurwkN"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMdjAcSrxb-q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i56aKA0K8adr"
      },
      "source": [
        "## Part 1\n",
        "Implement the function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. Our token definition is a single UTF-8 encoded character. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data.\n",
        "\n",
        "Note - do NOT lowercase the sentences in whi HW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws_5u7vRrg0o"
      },
      "outputs": [],
      "source": [
        "def preprocess() -> list[str]:\n",
        "  '''\n",
        "  Return a list of characters, representing the shared vocabulary of all languages\n",
        "  '''\n",
        "\n",
        "  tokens = set()\n",
        "  for filename in os.listdir(\"./data\"):\n",
        "    if filename.endswith(\".csv\"):\n",
        "      tweets = pd.read_csv(\"./data/\" + filename, usecols=['tweet_text'], encoding='utf-8')\n",
        "      chars = tweets['tweet_text'].apply(lambda x: {c for c in x})\n",
        "      tokens = tokens.union(*chars)\n",
        "\n",
        "  tokens.add('ה') # start token 'ה' for 'התחלה'\n",
        "  tokens.add('ס') # end token 'ס' for 'סוף'\n",
        "\n",
        "  return list(set(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpjtwHW08jyH"
      },
      "source": [
        "## Part 2\n",
        "Implement the function *lm* that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant *n*-1 sequences, and the values are dictionaries with the *n*_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{ \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25}, \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1} }\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uySEXdEUrkq_"
      },
      "outputs": [],
      "source": [
        "def lm(lang: str, n: int, smoothed: bool = False) -> dict[str, dict[str, float]]:\n",
        "  '''\n",
        "  Return a language model for the given lang and n-gram (n), with an option for smoothing.\n",
        "  :param lang: the language of the model\n",
        "  :param n: the n_gram value\n",
        "  :param smoothed: boolean indicating whether to apply smoothing\n",
        "  :return: a dictionary where the keys are n-1 grams and the values are dictionaries\n",
        "  '''\n",
        "\n",
        "  # Initialize a vocabulary\n",
        "  vocab = preprocess()\n",
        "\n",
        "\n",
        "\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "\n",
        "  # Read the tweets according to the input language\n",
        "  tweets = pd.read_csv(\"./data/\" + lang + \".csv\", usecols=['tweet_text'], encoding='utf-8')\n",
        "  total_length = 0\n",
        "\n",
        "\n",
        "  # Initialize a model as dicationary. handle the case where n = 1:\n",
        "  if n == 1:\n",
        "    model = defaultdict(lambda: 0.0)\n",
        "\n",
        "  else:\n",
        "    model = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
        "\n",
        "\n",
        "  # Iterate over each tweet\n",
        "  for tweet in tweets['tweet_text']:\n",
        "\n",
        "\n",
        "    text = ''.join(['ה']*(n-1) + [c for c in tweet] + ['ס'])\n",
        "\n",
        "    # Handle the unigram case separately - count the appearance of each character in the vocabulary\n",
        "    if n == 1:\n",
        "      for char in text:\n",
        "        model[char] += 1\n",
        "        total_length += 1\n",
        "\n",
        "    # for n>1 slice any n gram, and save in the model as key the n-1 gram, count the number of appearances of each n character\n",
        "    else:\n",
        "\n",
        "\n",
        "      for i in range(len(text) - n + 1):\n",
        "        seq = text[i:i+n-1]\n",
        "        next_char = text[i+n-1]\n",
        "        model[seq][next_char] += 1\n",
        "\n",
        "  # Apply add-one smoothing\n",
        "  if smoothed:\n",
        "    # For the unigram case apply the smoothing by add 1 and devide by vocabulary size\n",
        "    if n == 1:\n",
        "      for char in vocab:\n",
        "        model[char] += 1\n",
        "        model[char] /= total_length + vocab_size + 1\n",
        "\n",
        "      # Add 1 for unkown\n",
        "      model['ל'] = 1 / (total_length + vocab_size + 1)\n",
        "\n",
        "    else:\n",
        "      # Iterate over every key in the model for smoothing the values\n",
        "\n",
        "      for seq in model:\n",
        "          # Sum the values for the denominator\n",
        "          total_count = sum(model[seq].values())\n",
        "\n",
        "          # Recalculate the probabiltiy (the smoothing)\n",
        "          model[seq] = {char: (model[seq].get(char, 0) + 1) / (total_count + vocab_size) for char in vocab}\n",
        "\n",
        "      # Add unknown key with all vocabulray as potential character\n",
        "      model['ל'] = {char: 1 / vocab_size for char in vocab}\n",
        "\n",
        "\n",
        "  # Without smoothing\n",
        "  else:\n",
        "\n",
        "    # For the unigram case estimate the probabilty of each character\n",
        "    if n == 1:\n",
        "      # for char in vocab:\n",
        "      for char in model.keys():\n",
        "        model[char] /= total_length\n",
        "\n",
        "\n",
        "    else:\n",
        "      # Iterate over every n-1 gram and n'th token\n",
        "      for seq, next_chars in model.items():\n",
        "\n",
        "        # Count the number of appearances of the n-1 gram\n",
        "        seq_count = sum(next_chars.values())\n",
        "\n",
        "        # Iterate over every character in the Vocabulary and apply the smoothing (add 1 and devide by the vocabulary size)\n",
        "        # for char in vocab: (in case we want zeros)\n",
        "        # Add only existing probabilities\n",
        "        for char in next_chars:\n",
        "          model[seq][char] /= seq_count\n",
        "\n",
        "  return dict(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwZnk7Ke8rW5"
      },
      "source": [
        "## Part 3\n",
        "Implement the function *eval* that returns the perplexity of a model (dictionary) running over the data file of the given target language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6fzPRFtn6y_"
      },
      "outputs": [],
      "source": [
        "def eval(model: dict, target_lang: str, n: int) -> float:\n",
        "  '''\n",
        "  Return the perplexity value calculated over applying the model on the text file\n",
        "  of the target_lang language.\n",
        "  :param model: the language model\n",
        "  :param target_lang: the target language\n",
        "  :param n: The n-gram of the model\n",
        "  :return: the perplexity value\n",
        "  '''\n",
        "\n",
        "  perplexities = []\n",
        "\n",
        "  # List of keys - n-1 grams\n",
        "  model_sequences = list(model)\n",
        "\n",
        "  # Read the tweets\n",
        "  tweets = pd.read_csv(\"./data/\" + target_lang + \".csv\", usecols=['tweet_text'], encoding='utf-8')\n",
        "\n",
        "  # Iterate over all the tweets\n",
        "  for text in tweets['tweet_text']:\n",
        "\n",
        "    # List of logs of probabilities\n",
        "    tweet_probs = []\n",
        "\n",
        "    # For the unigram case iterate over every character in the text\n",
        "    if n == 1:\n",
        "      for char in text:\n",
        "\n",
        "        # For any character that is not in the text, add the probability of unknown character\n",
        "        if char not in model_sequences:\n",
        "          tweet_probs.append(np.log2(model['ל']))\n",
        "\n",
        "        # Add the propability of the current char\n",
        "        else:\n",
        "          tweet_probs.append(np.log2(model[char]))\n",
        "\n",
        "\n",
        "    # For n>1\n",
        "    else:\n",
        "      # Iterate over any possible probability  and add the log of the probability\n",
        "      tweet_probs.extend( np.log2(model['ל'][text[i+n-1]] if text[i:i+n-1] not in model_sequences else model[text[i:i+n-1]][text[i+n-1]])\n",
        "      for i in range(len(text) - n + 1))\n",
        "\n",
        "    # Calculate the entropy of the tweet\n",
        "    tweet_entropy = - np.mean(tweet_probs)\n",
        "\n",
        "    # Calculate the perplexity\n",
        "    perplexities.append(2 ** tweet_entropy)\n",
        "\n",
        "\n",
        "\n",
        "  return np.mean(perplexities)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZYVc7hB84LP"
      },
      "source": [
        "## Part 4\n",
        "Implement the *match* function that calls *eval* using a specific value of *n* for every possible language pair among the languages we have data for. You should call *eval* for every language pair four times, with each call assign a different value for *n* (1-4). Each language pair is composed of the source language and the target language. Before you make the call, you need to call the *lm* function to create the language model for the source language. Then you can call *eval* with the language model and the target language. The function should return a pandas DataFrame with the following four columns: *source_lang*, *target_lang*, *n*, *perplexity*. The values for the first two columns are the two-letter language codes. The value for *n* is the *n* you use for generating the specific perplexity values which you should store in the forth column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16ew9aZWroPC"
      },
      "outputs": [],
      "source": [
        "def match() -> pd.DataFrame:\n",
        "  '''\n",
        "  Return a DataFrame containing one line per every language pair and n_gram.\n",
        "  Each line will contain the perplexity calculated when applying the language model\n",
        "  of the source language on the text of the target language.\n",
        "  :return: a DataFrame containing the perplexity values\n",
        "  '''\n",
        "  columns = ['source', 'target', 'n', 'perplexity']\n",
        "  langs = ['en', 'fr', 'pt', 'es', 'tl', 'nl', 'in', 'it']\n",
        "\n",
        "  ns = [1,2,3,4]\n",
        "\n",
        "  results = []\n",
        "\n",
        "  # Iterate every n\n",
        "  for n in ns:\n",
        "    # Iterate over every permutation of 2 languages\n",
        "    for source in langs:\n",
        "      # Generate a model for the current source language\n",
        "      model = lm(source, n, smoothed=True)\n",
        "\n",
        "      for target in langs:\n",
        "\n",
        "      # Calculate the perplexity\n",
        "        perplexity = eval(model, target, n)\n",
        "\n",
        "        # Append the current permutation to the dataframe\n",
        "        results.append({'source': source, 'target': target, 'n': n, 'perplexity': perplexity})\n",
        "        print(f'source: {source}, target: {target}, n: {n}, perplexity: {perplexity}')\n",
        "\n",
        "\n",
        "  df = pd.DataFrame(results, columns=columns)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAQoR0dH9C3T"
      },
      "source": [
        "## Part 5\n",
        "Implement the *generate* function which takes a language code, *n*, the prompt (the starting text), the number of tokens to generate, and *r*, which is the random seed for any randomized action you plan to take in your implementation. The function should start generating tokens, one by one, using the language model of the given source language and *n*. The prompt should be used as a starting point for aligning on the probabilities to be used for generating the next token.\n",
        "\n",
        "Note - The generation of the next token should be from the LM's distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpCm24-RrpuA"
      },
      "outputs": [],
      "source": [
        "def generate(lang: str, n: int, prompt: str, number_of_tokens: int, r: int) -> str:\n",
        "  '''\n",
        "  Generate text in the given language using the given parameters.\n",
        "  :param lang: the language of the model\n",
        "  :param n: the n_gram value\n",
        "  :param prompt: the prompt to start the generation\n",
        "  :param number_of_tokens: the number of tokens to generate\n",
        "  :param r: the random seed to use\n",
        "  '''\n",
        "  # set the random seed\n",
        "  np.random.seed(r)\n",
        "  # create the language model\n",
        "  model = lm(lang, n, smoothed=False)\n",
        "  # generate the text\n",
        "  text = prompt\n",
        "\n",
        "  # For the unigrams case (different type of model)\n",
        "  if n==1:\n",
        "    for i in range(number_of_tokens):\n",
        "\n",
        "      # generate the next token\n",
        "      token = np.random.choice(list(model), p=list(model.values()))\n",
        "\n",
        "      # add the token to the text\n",
        "      text += token\n",
        "\n",
        "  else:\n",
        "    for i in range(number_of_tokens):\n",
        "\n",
        "      # generate the next token\n",
        "      # If the n-1 gram is not in the current model, use the unkown key to generate the next character\n",
        "      if text[(-n+1):] not in model.keys():\n",
        "        token = np.random.choice(list(model['ל']), p=list(model['ל'].values()))\n",
        "        print (f'{text[(-n+1):]} not in model ')\n",
        "      # Generate character according to the n-1 gram probabilities\n",
        "      else:\n",
        "        token = np.random.choice(list(model[text[(-n+1):]]), p=list(model[text[(-n+1):]].values()))\n",
        "\n",
        "      # add the token to the text\n",
        "      text += token\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUWX8Ugu9INH"
      },
      "source": [
        "## Part 6\n",
        "Play with your generate function, try to generate different texts in different language and various values of *n*. No need to submit anything of that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ykbMBzG9LWn"
      },
      "outputs": [],
      "source": [
        "print(generate('en', 1, \"I\", 20, 5))\n",
        "print(generate('en', 2, \"I am\", 20, 5))\n",
        "print(generate('en', 3, \"I am\", 20, 5))\n",
        "print(generate('en', 4, \"I Love\", 20, 5))\n",
        "print(generate('es', 2, \"Soy\", 20, 5))\n",
        "print(generate('es', 3, \"Soy\", 20, 5))\n",
        "print(generate('fr', 2, \"Je suis\", 20, 5))\n",
        "print(generate('fr', 3, \"Je suis\", 20, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kADDxjUwcItW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2jNlDISr9aL"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv48OCT_sIYW"
      },
      "source": [
        "Copy the content of the **tests.py** file from the repo and paste below. This will create the results.json file and download it to your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZTlc2ieruqq"
      },
      "outputs": [],
      "source": [
        "# Create tests\n",
        "def test_preprocess():\n",
        "    return {\n",
        "        'vocab_length': len(preprocess()),\n",
        "    }\n",
        "\n",
        "def test_lm():\n",
        "    return {\n",
        "        'english_2_gram_length': len(lm('en', 2, True)),\n",
        "        'english_3_gram_length': len(lm('en', 3, True)),\n",
        "        'french_3_gram_length': len(lm('fr', 3, True)),\n",
        "        'spanish_3_gram_length': len(lm('es', 3, True)),\n",
        "    }\n",
        "\n",
        "def test_eval():\n",
        "    return {\n",
        "        'en_en': eval(lm('en', 3, True), 'en', 3),\n",
        "        'en_fr': eval(lm('en', 3, True), 'fr', 3),\n",
        "        'en_tl': eval(lm('en', 3, True), 'tl', 3),\n",
        "        'en_nl': eval(lm('en', 3, True), 'nl', 3),\n",
        "    }\n",
        "\n",
        "def test_match():\n",
        "    df = match()\n",
        "    return {\n",
        "        'en_en_3': df[(df['source'] == 'en') & (df['target'] == 'en') & (df['n'] == 3)]['perplexity'].values[0],\n",
        "        'en_tl_3': df[(df['source'] == 'en') & (df['target'] == 'tl') & (df['n'] == 3)]['perplexity'].values[0],\n",
        "        'en_nl_3': df[(df['source'] == 'en') & (df['target'] == 'nl') & (df['n'] == 3)]['perplexity'].values[0],\n",
        "    }\n",
        "\n",
        "def test_generate():\n",
        "    return {\n",
        "        'english_1_gram': generate('en', 1, \"I\", 20, 5),\n",
        "        'english_2_gram': generate('en', 2, \"I am\", 20, 5),\n",
        "        'english_3_gram': generate('en', 3, \"I am\", 20, 5),\n",
        "        'english_4_gram': generate('en', 4, \"I Love\", 20, 5),\n",
        "        'spanish_2_gram': generate('es', 2, \"Soy\", 20, 5),\n",
        "        'spanish_3_gram': generate('es', 3, \"Soy\", 20, 5),\n",
        "        'french_2_gram': generate('fr', 2, \"Je suis\", 20, 5),\n",
        "        'french_3_gram': generate('fr', 3, \"Je suis\", 20, 5),\n",
        "    }\n",
        "\n",
        "TESTS = [test_preprocess, test_lm, test_eval, test_match, test_generate]\n",
        "\n",
        "# Run tests and save results\n",
        "res = {}\n",
        "for test in TESTS:\n",
        "    try:\n",
        "        cur_res = test()\n",
        "        res.update({test.__name__: cur_res})\n",
        "    except Exception as e:\n",
        "        res.update({test.__name__: repr(e)})\n",
        "\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "\n",
        "# Download the results.json file\n",
        "files.download('results.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the local files, results.json should be there now and\n",
        "# also downloaded to your local machine\n",
        "!ls -l"
      ],
      "metadata": {
        "id": "MGEroQZE-hoL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}